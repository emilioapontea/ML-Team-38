{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-A3RFpmWISto",
    "outputId": "d43f95d7-068f-4996-9e31-b7dfaff95f6f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import userdata\n",
    "\n",
    "# # Clone the entire repo.\n",
    "# username = 'emilioapontea'\n",
    "# token = userdata.get(\"token\")\n",
    "# repo_name = 'ML-Team-38'\n",
    "\n",
    "# !git clone https://{username}:{token}@github.com/{username}/{repo_name}.git\n",
    "\n",
    "\n",
    "# %cd {repo_name}\n",
    "# !git pull\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rjMxaI8UTtg",
    "outputId": "3ddaba7d-33c0-492e-ad13-bd35a1d51b75"
   },
   "outputs": [],
   "source": [
    "# %cd ML-Team-38\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x7U0uS87IHP7"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from preprocessing import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from skimage import feature\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E74gi4k-IHP9"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "dataset = ImageFolder(root=\"./split_dataset/train\", transform=transform)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdPzccggIHP9",
    "outputId": "5a749898-57e9-413c-c7b6-b82a855b2bae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorguyard/miniconda3/envs/ml_project/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/victorguyard/miniconda3/envs/ml_project/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True) # RESNET18\n",
    "# model = torchvision.models.resnet50(pretrained=True) # RESNET50\n",
    "# model = torchvision.models.vgg16(pretrained=True) #VGG16\n",
    "# model =  torchvision.models.inception_v3(pretrained=True) # INCEPTION V3\n",
    "in_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(in_features, num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1vUUDuvjIHP-"
   },
   "outputs": [],
   "source": [
    "numEpochs = 3\n",
    "\n",
    "def testAccuracy(dataPath, model):\n",
    "  testset = ImageFolder(root=dataPath, transform=transform)\n",
    "  testloader = torch.utils.data.DataLoader(testset, batch_size=10,\n",
    "                                         shuffle=False)\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "      for data in testloader:\n",
    "          images, labels = data\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print(f'Accuracy of the network on the images {100 * correct / total} on {dataPath}')\n",
    "  print('Accuracy of the network on the images: %d %%' % (\n",
    "      100 * correct / total))\n",
    "\n",
    "test_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "def trainModel():\n",
    "    i = 0\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for epoch in range(numEpochs):\n",
    "        for images, labels in dataloader:\n",
    "            images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Training: {epoch} {i}\")\n",
    "            i += 1\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        print(f\"Evaluating on Train: {epoch}\")\n",
    "        test_acc_history.append(testAccuracy(\"./split_dataset/train\", model))\n",
    "        print(f\"Evaluating on Val: {epoch}\")\n",
    "        val_acc_history.append(testAccuracy(\"./split_dataset/val\", model))\n",
    "        model.train()\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{numEpochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuVrL2BWIHP-",
    "outputId": "ed4e1375-d276-4154-b2c9-366c75450e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0 0\n",
      "Training: 0 1\n",
      "Training: 0 2\n",
      "Training: 0 3\n",
      "Training: 0 4\n",
      "Training: 0 5\n",
      "Training: 0 6\n",
      "Training: 0 7\n",
      "Training: 0 8\n",
      "Training: 0 9\n",
      "Training: 0 10\n",
      "Training: 0 11\n",
      "Training: 0 12\n",
      "Training: 0 13\n",
      "Training: 0 14\n",
      "Training: 0 15\n",
      "Training: 0 16\n",
      "Training: 0 17\n",
      "Training: 0 18\n",
      "Training: 0 19\n",
      "Training: 0 20\n",
      "Training: 0 21\n",
      "Training: 0 22\n",
      "Training: 0 23\n",
      "Training: 0 24\n",
      "Training: 0 25\n",
      "Training: 0 26\n",
      "Training: 0 27\n",
      "Training: 0 28\n",
      "Training: 0 29\n",
      "Training: 0 30\n",
      "Training: 0 31\n",
      "Training: 0 32\n",
      "Training: 0 33\n",
      "Training: 0 34\n",
      "Training: 0 35\n",
      "Training: 0 36\n",
      "Training: 0 37\n",
      "Training: 0 38\n",
      "Training: 0 39\n",
      "Evaluating on Train: 0\n",
      "Accuracy of the network on the images 32.99373040752351 on ./split_dataset/train\n",
      "Accuracy of the network on the images: 32 %\n",
      "Evaluating on Val: 0\n",
      "Accuracy of the network on the images 32.474964234620884 on ./split_dataset/val\n",
      "Accuracy of the network on the images: 32 %\n",
      "Epoch [1/5], Loss: 1.3372\n",
      "Training: 1 40\n",
      "Training: 1 41\n",
      "Training: 1 42\n",
      "Training: 1 43\n",
      "Training: 1 44\n",
      "Training: 1 45\n",
      "Training: 1 46\n",
      "Training: 1 47\n",
      "Training: 1 48\n",
      "Training: 1 49\n",
      "Training: 1 50\n",
      "Training: 1 51\n",
      "Training: 1 52\n",
      "Training: 1 53\n",
      "Training: 1 54\n",
      "Training: 1 55\n",
      "Training: 1 56\n",
      "Training: 1 57\n",
      "Training: 1 58\n",
      "Training: 1 59\n",
      "Training: 1 60\n",
      "Training: 1 61\n",
      "Training: 1 62\n",
      "Training: 1 63\n",
      "Training: 1 64\n",
      "Training: 1 65\n",
      "Training: 1 66\n",
      "Training: 1 67\n",
      "Training: 1 68\n",
      "Training: 1 69\n",
      "Training: 1 70\n",
      "Training: 1 71\n",
      "Training: 1 72\n",
      "Training: 1 73\n",
      "Training: 1 74\n",
      "Training: 1 75\n",
      "Training: 1 76\n",
      "Training: 1 77\n",
      "Training: 1 78\n",
      "Training: 1 79\n",
      "Evaluating on Train: 1\n",
      "Accuracy of the network on the images 55.289968652037615 on ./split_dataset/train\n",
      "Accuracy of the network on the images: 55 %\n",
      "Evaluating on Val: 1\n",
      "Accuracy of the network on the images 49.78540772532189 on ./split_dataset/val\n",
      "Accuracy of the network on the images: 49 %\n",
      "Epoch [2/5], Loss: 1.3607\n",
      "Training: 2 80\n",
      "Training: 2 81\n",
      "Training: 2 82\n",
      "Training: 2 83\n",
      "Training: 2 84\n",
      "Training: 2 85\n",
      "Training: 2 86\n",
      "Training: 2 87\n",
      "Training: 2 88\n",
      "Training: 2 89\n",
      "Training: 2 90\n",
      "Training: 2 91\n",
      "Training: 2 92\n",
      "Training: 2 93\n",
      "Training: 2 94\n",
      "Training: 2 95\n",
      "Training: 2 96\n",
      "Training: 2 97\n",
      "Training: 2 98\n",
      "Training: 2 99\n",
      "Training: 2 100\n",
      "Training: 2 101\n",
      "Training: 2 102\n",
      "Training: 2 103\n",
      "Training: 2 104\n",
      "Training: 2 105\n",
      "Training: 2 106\n",
      "Training: 2 107\n",
      "Training: 2 108\n",
      "Training: 2 109\n",
      "Training: 2 110\n",
      "Training: 2 111\n",
      "Training: 2 112\n",
      "Training: 2 113\n",
      "Training: 2 114\n",
      "Training: 2 115\n",
      "Training: 2 116\n",
      "Training: 2 117\n",
      "Training: 2 118\n",
      "Training: 2 119\n",
      "Evaluating on Train: 2\n",
      "Accuracy of the network on the images 46.51253918495298 on ./split_dataset/train\n",
      "Accuracy of the network on the images: 46 %\n",
      "Evaluating on Val: 2\n",
      "Accuracy of the network on the images 37.625178826895564 on ./split_dataset/val\n",
      "Accuracy of the network on the images: 37 %\n",
      "Epoch [3/5], Loss: 1.0130\n",
      "Training: 3 120\n",
      "Training: 3 121\n",
      "Training: 3 122\n",
      "Training: 3 123\n",
      "Training: 3 124\n",
      "Training: 3 125\n",
      "Training: 3 126\n",
      "Training: 3 127\n",
      "Training: 3 128\n",
      "Training: 3 129\n",
      "Training: 3 130\n",
      "Training: 3 131\n",
      "Training: 3 132\n",
      "Training: 3 133\n",
      "Training: 3 134\n",
      "Training: 3 135\n",
      "Training: 3 136\n",
      "Training: 3 137\n",
      "Training: 3 138\n",
      "Training: 3 139\n",
      "Training: 3 140\n",
      "Training: 3 141\n",
      "Training: 3 142\n",
      "Training: 3 143\n",
      "Training: 3 144\n",
      "Training: 3 145\n",
      "Training: 3 146\n",
      "Training: 3 147\n",
      "Training: 3 148\n",
      "Training: 3 149\n",
      "Training: 3 150\n",
      "Training: 3 151\n",
      "Training: 3 152\n",
      "Training: 3 153\n",
      "Training: 3 154\n",
      "Training: 3 155\n",
      "Training: 3 156\n",
      "Training: 3 157\n",
      "Training: 3 158\n",
      "Training: 3 159\n",
      "Evaluating on Train: 3\n",
      "Accuracy of the network on the images 66.88871473354232 on ./split_dataset/train\n",
      "Accuracy of the network on the images: 66 %\n",
      "Evaluating on Val: 3\n",
      "Accuracy of the network on the images 54.506437768240346 on ./split_dataset/val\n",
      "Accuracy of the network on the images: 54 %\n",
      "Epoch [4/5], Loss: 0.8232\n",
      "Training: 4 160\n",
      "Training: 4 161\n",
      "Training: 4 162\n",
      "Training: 4 163\n",
      "Training: 4 164\n",
      "Training: 4 165\n",
      "Training: 4 166\n",
      "Training: 4 167\n",
      "Training: 4 168\n",
      "Training: 4 169\n",
      "Training: 4 170\n",
      "Training: 4 171\n",
      "Training: 4 172\n",
      "Training: 4 173\n",
      "Training: 4 174\n",
      "Training: 4 175\n",
      "Training: 4 176\n",
      "Training: 4 177\n",
      "Training: 4 178\n",
      "Training: 4 179\n",
      "Training: 4 180\n",
      "Training: 4 181\n",
      "Training: 4 182\n",
      "Training: 4 183\n",
      "Training: 4 184\n",
      "Training: 4 185\n",
      "Training: 4 186\n",
      "Training: 4 187\n",
      "Training: 4 188\n",
      "Training: 4 189\n",
      "Training: 4 190\n",
      "Training: 4 191\n",
      "Training: 4 192\n",
      "Training: 4 193\n",
      "Training: 4 194\n",
      "Training: 4 195\n",
      "Training: 4 196\n",
      "Training: 4 197\n",
      "Training: 4 198\n",
      "Training: 4 199\n",
      "Evaluating on Train: 4\n",
      "Accuracy of the network on the images 59.1692789968652 on ./split_dataset/train\n",
      "Accuracy of the network on the images: 59 %\n",
      "Evaluating on Val: 4\n",
      "Accuracy of the network on the images 47.92560801144492 on ./split_dataset/val\n",
      "Accuracy of the network on the images: 47 %\n",
      "Epoch [5/5], Loss: 0.6088\n"
     ]
    }
   ],
   "source": [
    "model = trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gyThYEEqIHP-",
    "outputId": "71984cc9-439f-4240-9f5a-4a874fb9a7a8"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m dataiter\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__next__\u001b[39m()\n\u001b[1;32m      5\u001b[0m classes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m classes[predicted[j]]\n\u001b[1;32m     13\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "dataiter = iter(dataloader)\n",
    "images, labels = dataiter.__next__()\n",
    "\n",
    "\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "outputs = model(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "# print images\n",
    "plt.imshow(torchvision.utils.make_grid(images).permute(1, 2, 0))\n",
    "print('GroundTruth: ', ' '.join('%s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "goVOuSTNIHP-"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/resnet-18.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './models/resnet-18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (3266921800.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    generate_and_plot_confusion_matrix(\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from vis_utils import *\n",
    "\n",
    "train_acc = [32.05329153605015, 19.905956112852664, 36.52037617554859, 41.73197492163009, 65.8307210031348]\n",
    "val_acc = [31.044349070100143, 20.457796852646638, 31.044349070100143, 36.76680972818312, 51.93133047210301]\n",
    "\n",
    "plot_acc_history(train_acc, val_acc)\n",
    "plot_acc_history(test_acc_history, test_val_history)\n",
    "\n",
    "generate_and_plot_confusion_matrix("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
